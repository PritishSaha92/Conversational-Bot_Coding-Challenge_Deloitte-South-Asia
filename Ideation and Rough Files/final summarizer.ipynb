{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11301183,"sourceType":"datasetVersion","datasetId":7067443},{"sourceId":11309777,"sourceType":"datasetVersion","datasetId":7073389},{"sourceId":11311208,"sourceType":"datasetVersion","datasetId":7074482}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:08:47.621593Z","iopub.execute_input":"2025-04-07T13:08:47.621856Z","iopub.status.idle":"2025-04-07T13:08:53.574254Z","shell.execute_reply.started":"2025-04-07T13:08:47.621825Z","shell.execute_reply":"2025-04-07T13:08:53.573224Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.2.1)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.3.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.47.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.11.12)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nDownloading trl-0.16.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.16.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:08:53.575209Z","iopub.execute_input":"2025-04-07T13:08:53.575456Z","iopub.status.idle":"2025-04-07T13:08:56.915678Z","shell.execute_reply.started":"2025-04-07T13:08:53.575434Z","shell.execute_reply":"2025-04-07T13:08:56.914617Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:08:56.916783Z","iopub.execute_input":"2025-04-07T13:08:56.917140Z","iopub.status.idle":"2025-04-07T13:09:00.319442Z","shell.execute_reply.started":"2025-04-07T13:08:56.917103Z","shell.execute_reply":"2025-04-07T13:09:00.318411Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:00.321551Z","iopub.execute_input":"2025-04-07T13:09:00.321812Z","iopub.status.idle":"2025-04-07T13:09:06.494270Z","shell.execute_reply.started":"2025-04-07T13:09:00.321789Z","shell.execute_reply":"2025-04-07T13:09:06.493274Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport torch\nimport re\nimport ast\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:06.495497Z","iopub.execute_input":"2025-04-07T13:09:06.495797Z","iopub.status.idle":"2025-04-07T13:09:17.306099Z","shell.execute_reply.started":"2025-04-07T13:09:06.495764Z","shell.execute_reply":"2025-04-07T13:09:17.305191Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Get HuggingFace token\ndef get_hf_token():\n    user_secrets = UserSecretsClient()\n    return user_secrets.get_secret(\"HF_TOKEN\")\n\ndef safe_eval(s):\n    \"\"\"Safely evaluate a string as a Python expression\"\"\"\n    if not isinstance(s, str):\n        return s\n    try:\n        return ast.literal_eval(s)\n    except (SyntaxError, ValueError):\n        return s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.309371Z","iopub.execute_input":"2025-04-07T13:09:17.309679Z","iopub.status.idle":"2025-04-07T13:09:17.315076Z","shell.execute_reply.started":"2025-04-07T13:09:17.309647Z","shell.execute_reply":"2025-04-07T13:09:17.314224Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_employee_data(file_path=\"/kaggle/input/distressed-employees/distressed_employees_new.csv\"):\n    \"\"\"Load and process employee data from CSV\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"File not found at {file_path}\")\n        \n        df = pd.read_csv(file_path)\n        \n        # Convert string representations of lists to actual lists\n        for col in ['Problems', 'Other Problems']:\n            if col in df.columns:\n                df[col] = df[col].apply(lambda x: safe_eval(x) if isinstance(x, str) else x)\n        \n        return df\n    except Exception as e:\n        print(f\"Error loading employee data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.317602Z","iopub.execute_input":"2025-04-07T13:09:17.317797Z","iopub.status.idle":"2025-04-07T13:09:17.352160Z","shell.execute_reply.started":"2025-04-07T13:09:17.317780Z","shell.execute_reply":"2025-04-07T13:09:17.351404Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def load_model_and_tokenizer():\n    \"\"\"Load the Mistral AI model and tokenizer with 4-bit quantization\"\"\"\n    print(\"Loading Mistral model...\")\n    hf_token = get_hf_token()\n    \n    compute_dtype = torch.float16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.2\",\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        token=hf_token\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.2\",\n        trust_remote_code=True,\n        padding_side=\"left\",\n        token=hf_token\n    )\n    \n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.353373Z","iopub.execute_input":"2025-04-07T13:09:17.353680Z","iopub.status.idle":"2025-04-07T13:09:17.371089Z","shell.execute_reply.started":"2025-04-07T13:09:17.353652Z","shell.execute_reply":"2025-04-07T13:09:17.370508Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def extract_conversations_from_qna(chat_history):\n    \"\"\"Extract and format conversations from chat history\"\"\"\n    if not chat_history:\n        return \"No chat history available\"\n        \n    if isinstance(chat_history, str):\n        # Check if it's a file path\n        if os.path.exists(chat_history):\n            try:\n                with open(chat_history, 'r') as f:\n                    chat_history = json.load(f)\n            except json.JSONDecodeError:\n                with open(chat_history, 'r') as f:\n                    return f.read()\n        else:\n            # Try to parse as JSON string\n            try:\n                chat_history = json.loads(chat_history)\n            except json.JSONDecodeError:\n                return chat_history\n\n    # Process based on structure\n    formatted_chat = \"\"\n    if isinstance(chat_history, list):\n        for entry in chat_history:\n            if isinstance(entry, dict):\n                # Handle the specific format with direction and message\n                if 'direction' in entry and 'message' in entry:\n                    direction = entry.get('direction', '')\n                    message = entry.get('message', '')\n                    if direction == 'sent':\n                        formatted_chat += f\"User: {message}\\n\\n\"\n                    elif direction == 'received':\n                        formatted_chat += f\"Bot: {message}\\n\\n\"\n                # Handle standard chat format with role and content\n                elif 'role' in entry and 'content' in entry:\n                    role = entry.get('role', '')\n                    content = entry.get('content', '')\n                    formatted_chat += f\"{role.capitalize()}: {content}\\n\\n\"\n                # Handle QnA format\n                elif 'question' in entry and 'answer' in entry:\n                    question = entry.get('question', '')\n                    answer = entry.get('answer', '')\n                    formatted_chat += f\"User: {question}\\nBot: {answer}\\n\\n\"\n                # Handle other formats\n                elif any(key in entry for key in ['user', 'bot', 'assistant', 'system']):\n                    for key, value in entry.items():\n                        if isinstance(value, str) and value.strip():\n                            formatted_chat += f\"{key.capitalize()}: {value}\\n\\n\"\n            elif isinstance(entry, str):\n                formatted_chat += f\"{entry}\\n\\n\"\n    elif isinstance(chat_history, dict):\n        # Handle different dictionary formats\n        if 'messages' in chat_history:\n            return extract_conversations_from_qna(chat_history['messages'])\n        elif 'history' in chat_history:\n            return extract_conversations_from_qna(chat_history['history'])\n        else:\n            for key, value in chat_history.items():\n                if isinstance(value, str) and value.strip():\n                    formatted_chat += f\"{key.capitalize()}: {value}\\n\\n\"\n    else:\n        formatted_chat = str(chat_history)\n    \n    return formatted_chat.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:34:31.521442Z","iopub.execute_input":"2025-04-07T13:34:31.521837Z","iopub.status.idle":"2025-04-07T13:34:31.530697Z","shell.execute_reply.started":"2025-04-07T13:34:31.521805Z","shell.execute_reply":"2025-04-07T13:34:31.529955Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def format_employee_data(employee_data):\n    \"\"\"Format employee data for the prompt\"\"\"\n    formatted_data = {}\n    \n    # Basic info\n    formatted_data[\"Employee ID\"] = employee_data.get(\"Employee_ID\", \"\")\n    formatted_data[\"Average Work Hours\"] = employee_data.get(\"Average Work Hours\", \"\")\n    formatted_data[\"Performance Rating\"] = employee_data.get(\"Performance Rating\", \"\")\n    formatted_data[\"Reward Factor\"] = employee_data.get(\"Reward Factor\", \"\")\n    formatted_data[\"Vibe Factor\"] = employee_data.get(\"Vibe Factor\", \"\")\n    formatted_data[\"Anomaly Score\"] = employee_data.get(\"Anamaly_Score\", \"\")\n    \n    # Problems\n    formatted_data[\"Problems\"] = []\n    problems = employee_data.get(\"Problems\", [])\n    if problems and isinstance(problems, list):\n        for problem in problems:\n            if isinstance(problem, list) and len(problem) >= 2:\n                formatted_data[\"Problems\"].append({\n                    \"issue\": problem[0],\n                    \"score\": problem[1]\n                })\n    \n    # Other Problems\n    formatted_data[\"Other Problems\"] = []\n    other_problems = employee_data.get(\"Other Problems\", [])\n    if other_problems and isinstance(other_problems, list):\n        for problem in other_problems:\n            if isinstance(problem, list) and len(problem) >= 2:\n                formatted_data[\"Other Problems\"].append({\n                    \"issue\": problem[0],\n                    \"score\": problem[1]\n                })\n    \n    return json.dumps(formatted_data, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.399887Z","iopub.execute_input":"2025-04-07T13:09:17.400077Z","iopub.status.idle":"2025-04-07T13:09:17.425866Z","shell.execute_reply.started":"2025-04-07T13:09:17.400059Z","shell.execute_reply":"2025-04-07T13:09:17.425217Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def generate_hr_report(model, tokenizer, employee_id, employee_data, chat_history):\n    \"\"\"Generate HR report using the Mistral model\"\"\"\n    processed_chat = extract_conversations_from_qna(chat_history)\n    formatted_data = format_employee_data(employee_data)\n    \n    # Create the prompt for Mistral format\n    prompt = f\"\"\"<s>[INST]\n    You are an AI assistant that analyzes conversations between employees and a chatbot, then creates comprehensive HR reports based on distressed employee data. Your task is to:\n    \n    1. Analyze conversation tone/content\n    2. Identify employee concerns\n    3. Extract engagement/satisfaction insights\n    4. Assess risks\n    5. Recommend actionable steps\n    \n    CRITICAL INSTRUCTION: NEVER use \"N/A\" or placeholder text anywhere in your report. If data appears missing, make reasonable inferences based on available information or provide generic but meaningful content.\n    \n    Create a STRUCTURED report with these REQUIRED sections:\n    \n    ## ðŸ’¼ Employee Summary\n    â€¢ Basic Info:\n      - Employee ID: {employee_id}\n      - Problem indicators (from Problems and Other Problems columns)\n      - Work hours and performance metrics\n    â€¢ Quantitative Metrics:\n      - Anomaly Score interpretation\n      - Vibe Factor analysis\n      - Reward Factor evaluation\n      - Performance Rating context\n      - Average Work Hours assessment\n    â€¢ Qualitative Analysis:\n      - Top issues identified in Problems column\n      - Secondary issues from Other Problems column\n      - Risk patterns and correlations\n    \n    ## ðŸ” Key Insights\n    â€¢ Technical Observations:\n      - Analyze highest-scoring problem factors \n      - Identify patterns across problem indicators\n    â€¢ Quantitative Analysis:\n      - Compare metrics to normal ranges\n      - Analyze correlation between Problems and numeric indicators\n    \n    ## ðŸš¨ Risk Assessment\n    â€¢ Concerns:\n      - Low-level concerns (scores 0.1-0.3)\n      - Medium-level concerns (scores 0.3-0.6)\n      - High-level concerns (scores >0.6)\n    â€¢ Anomalies:\n      - Interpret Anomaly Score relative to company average\n      - Analyze unusual patterns in Problems and Other Problems\n    â€¢ Indicators:\n      - Connect conversation content with identified problem areas\n      - Highlight behavioral indicators from chat history\n    \n    ## ðŸ“ˆ Recommended Actions\n    â€¢ Critical Steps:\n      - 3-5 highest priority actions targeting top problem areas\n      - Specific responsibility assignments (HR/manager/employee)\n      - Clear timelines for implementation\n    â€¢ Additional Considerations:\n      - Follow-up procedures based on Anomaly Score\n      - Support resources for identified Problems\n      - Preventive measures for potential issues\n    \n    FORMATTING RULES:\n    âœ… Use EXACT section headers with emojis as shown above\n    âœ… Bold subsection headers using ** **\n    âœ… Each bullet point must contain 15+ meaningful words\n    âœ… Use â€¢ for main points and + for subpoints\n    âœ… Include ALL data points from employee information\n    \n    CRITICAL REQUIREMENTS:\n    âœ… NO PLACEHOLDERS - never use \"N/A,\" \"TBD,\" or similar text\n    âœ… Balanced section lengths\n    âœ… Complete action items addressing ALL risks\n    âœ… Specific timelines and responsibility assignments\n    \n    If you're running out of space, be more concise in earlier sections but NEVER use placeholder text or leave sections incomplete.\n    <</SYS>>\n    \n    Employee: {employee_id}\n    Data: {formatted_data}\n    Conversation: {processed_chat}\n    \n    Generate a COMPLETE HR report with NO placeholder text like \"N/A\". Every section must contain meaningful content based on the distressed employees dataset structure and chat history. [/INST]\n    \"\"\"\n    \n    # Generate the report\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    output = model.generate(\n        **inputs,\n        max_length=4096,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.15\n    )\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    # Extract just the response part (after the prompt)\n    response_parts = response.split(\"[/INST]\")\n    if len(response_parts) > 1:\n        response = response_parts[-1].strip()\n    else:\n        # If splitting failed, try to extract the report content\n        start_idx = response.find(\"## ðŸ’¼ Employee Summary\")\n        if start_idx != -1:\n            response = response[start_idx:].strip()\n    \n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:43:12.650692Z","iopub.execute_input":"2025-04-07T13:43:12.651038Z","iopub.status.idle":"2025-04-07T13:43:12.658025Z","shell.execute_reply.started":"2025-04-07T13:43:12.651005Z","shell.execute_reply":"2025-04-07T13:43:12.657143Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def convert_markdown_to_structured_json(markdown_text):\n    \"\"\"Convert markdown HR report to structured JSON using line-by-line parsing\"\"\"\n    # Initialize the structure\n    report = {\n        \"Employee Summary\": {\n            \"Basic Info\": [],\n            \"Quantitative Metrics\": [],\n            \"Qualitative Metrics\": []\n        },\n        \"Key Insights\": {\n            \"Technical Observations\": [],\n            \"Quantitative Analysis\": []\n        },\n        \"Risk Assessment\": {\n            \"Concerns\": [],\n            \"Anomalies\": [],\n            \"Indicators\": []\n        },\n        \"Recommended Actions\": {\n            \"Critical Steps\": [],\n            \"Additional Considerations\": []\n        }\n    }\n    \n    # Clean the markdown text by removing indentation\n    cleaned_lines = [line.lstrip() for line in markdown_text.split('\\n')]\n    \n    current_section = None\n    current_subsection = None\n    \n    for line in cleaned_lines:\n        if not line:\n            continue\n            \n        # Check for main section headers\n        if line.startswith('## '):\n            section_title = line[3:].strip()\n            clean_title = re.sub(r'[^\\w\\s]', '', section_title).strip()\n            \n            for key in report.keys():\n                if clean_title.lower() in key.lower() or key.lower() in clean_title.lower():\n                    current_section = key\n                    current_subsection = None\n                    break\n                \n        # Check for subsection headers\n        elif line.startswith('### '):\n            if not current_section:\n                continue\n                \n            subsection_title = line[4:].strip()\n            if subsection_title.endswith(':'):\n                subsection_title = subsection_title[:-1]\n                \n            # Find matching subsection\n            for key in report[current_section].keys():\n                if subsection_title.lower() in key.lower() or key.lower() in subsection_title.lower():\n                    current_subsection = key\n                    break\n        \n        # Check for bullet points (handles different styles)\n        elif (line.startswith('-') or line.startswith('â€¢') or line.startswith('+') or \n              line.startswith('1.') or line.startswith('2.') or line.startswith('3.') or \n              line.startswith('4.') or line.startswith('5.')):\n            \n            if not current_section or not current_subsection:\n                continue\n                \n            # Extract bullet text (everything after the bullet marker)\n            marker_end = line.find(' ')\n            if marker_end != -1:\n                bullet_text = line[marker_end+1:].strip()\n                if bullet_text and bullet_text != \"**\":\n                    # Handle cases where there might be bold formatting\n                    if bullet_text.startswith('**') and '**:' in bullet_text:\n                        # Skip subsection headers in bullet format\n                        continue\n                    report[current_section][current_subsection].append(bullet_text)\n    \n    # Add this code here, just before the return statement\n    # Remove empty arrays from the report\n    for section, subsections in report.items():\n        for subsection in list(subsections.keys()):\n            if not subsections[subsection]:\n                del subsections[subsection]\n    \n    return report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:02:02.580559Z","iopub.execute_input":"2025-04-07T14:02:02.580898Z","iopub.status.idle":"2025-04-07T14:02:02.590149Z","shell.execute_reply.started":"2025-04-07T14:02:02.580872Z","shell.execute_reply":"2025-04-07T14:02:02.589359Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def save_report_to_json(report_data, employee_id, output_dir=\"./reports\"):\n    \"\"\"Save the generated report to a JSON file\"\"\"\n    # Create output directory if it doesn't exist\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    \n    # Create filename\n    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = output_path / f\"hr_report_{employee_id}_{timestamp}.json\"\n    \n    # Save JSON\n    with open(filename, 'w') as f:\n        json.dump(report_data, f, indent=2)\n    \n    print(f\"Report saved to {filename}\")\n    return str(filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.476710Z","iopub.execute_input":"2025-04-07T13:09:17.476927Z","iopub.status.idle":"2025-04-07T13:09:17.497399Z","shell.execute_reply.started":"2025-04-07T13:09:17.476907Z","shell.execute_reply":"2025-04-07T13:09:17.496834Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def generate_hr_summary(employee_id, chat_history=None, output_dir=\"./reports\"):\n    \"\"\"End-to-end process to generate HR summary\"\"\"\n    print(f\"Generating HR summary for employee {employee_id}\")\n    \n    # Load employee data\n    file_path = \"/kaggle/input/distressed-employees/distressed_employees_new.csv\"\n    df = load_employee_data(file_path)\n    \n    if df is None:\n        return {\"error\": \"Failed to load employee data\"}\n    \n    # Filter data for the specified employee\n    employee_rows = df[df['Employee_ID'] == employee_id]\n    \n    if len(employee_rows) == 0:\n        return {\"error\": f\"Employee ID {employee_id} not found in the dataset\"}\n    \n    employee_data = employee_rows.iloc[0].to_dict()\n    \n    # Load model and tokenizer\n    model, tokenizer = load_model_and_tokenizer()\n    \n    # Generate report\n    markdown_report = generate_hr_report(model, tokenizer, employee_id, employee_data, chat_history)\n    \n    # Convert markdown to structured JSON\n    structured_report = convert_markdown_to_structured_json(markdown_report)\n    \n    # Prepare final report\n    final_report = {\n        \"employee_id\": employee_id,\n        \"report_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"report\": structured_report,\n        \"raw_markdown\": markdown_report\n    }\n    \n    # Save report\n    report_file = save_report_to_json(final_report, employee_id, output_dir)\n    \n    return {\n        \"status\": \"success\",\n        \"employee_id\": employee_id,\n        \"report_file\": report_file\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:18.005414Z","iopub.execute_input":"2025-04-07T13:09:18.005731Z","iopub.status.idle":"2025-04-07T13:09:18.011510Z","shell.execute_reply.started":"2025-04-07T13:09:18.005707Z","shell.execute_reply":"2025-04-07T13:09:18.010572Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Main execution function with command line arguments\ndef main():\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Generate HR summary reports')\n    parser.add_argument('--employee_id', type=str, required=True, help='Employee ID')\n    parser.add_argument('--chat_file', type=str, help='Path to chat history JSON file')\n    parser.add_argument('--output_dir', type=str, default='./reports', help='Output directory for reports')\n    \n    args = parser.parse_args()\n    \n    # Generate report\n    result = generate_hr_summary(args.employee_id, args.chat_file, args.output_dir)\n    print(json.dumps(result, indent=2))\n\n# Replace the main() function with this code\nif __name__ == \"__main__\":\n    # Get employee ID directly from input\n    employee_id = input(\"Enter Employee ID (e.g., EMP0418): \")\n    \n    # Optional: Get chat file path (or leave empty)\n    chat_file = input(\"Enter chat file path (optional, press Enter to skip): \") or None\n    \n    # Use default output directory or customize\n    output_dir = \"./reports\"\n    \n    # Generate report\n    result = generate_hr_summary(employee_id, chat_file, output_dir)\n    print(json.dumps(result, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:02:08.338285Z","iopub.execute_input":"2025-04-07T14:02:08.338597Z","iopub.status.idle":"2025-04-07T14:04:20.672733Z","shell.execute_reply.started":"2025-04-07T14:02:08.338570Z","shell.execute_reply":"2025-04-07T14:04:20.671961Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter Employee ID (e.g., EMP0418):  EMP0040\nEnter chat file path (optional, press Enter to skip):  /kaggle/input/chat-history/chat_history.json\n"},{"name":"stdout","text":"Generating HR summary for employee EMP0040\nLoading Mistral model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4560b1b8ac11409aa48f9b0d479808a5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Report saved to reports/hr_report_EMP0040_20250407_140420.json\n{\n  \"status\": \"success\",\n  \"employee_id\": \"EMP0040\",\n  \"report_file\": \"reports/hr_report_EMP0040_20250407_140420.json\"\n}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom kaggle_secrets import UserSecretsClient\n\ndef get_hf_token():\n    \"\"\"Get HuggingFace token from Kaggle secrets\"\"\"\n    user_secrets = UserSecretsClient()\n    return user_secrets.get_secret(\"HF_TOKEN\")\n\ndef download_and_save_model(model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", output_dir=\"saved_model\", use_token=True):\n    \"\"\"\n    Downloads a model from Hugging Face and saves it to a local directory\n    \n    Args:\n        model_name: HuggingFace model name\n        output_dir: Directory to save the model\n        use_token: Whether to use HF token from Kaggle secrets\n    \"\"\"\n    print(f\"Downloading model {model_name}...\")\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Get HF token if needed\n    hf_token = get_hf_token() if use_token else None\n    \n    # Configure quantization for memory efficiency\n    compute_dtype = torch.float16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    )\n    \n    # Download model\n    print(\"Loading Mistral model...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        token=hf_token\n    )\n    \n    # Save the model to local directory\n    model_path = os.path.join(output_dir, \"model\")\n    os.makedirs(model_path, exist_ok=True)\n    model.save_pretrained(model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    # Download and save tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        padding_side=\"left\",\n        token=hf_token\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer_path = os.path.join(output_dir, \"tokenizer\")\n    os.makedirs(tokenizer_path, exist_ok=True)\n    tokenizer.save_pretrained(tokenizer_path)\n    print(f\"Tokenizer saved to {tokenizer_path}\")\n    \n    print(f\"Model and tokenizer saved to {output_dir}\")\n    return model, tokenizer, output_dir\n\ndef load_saved_model(model_dir=\"saved_model\"):\n    \"\"\"\n    Loads a previously downloaded model and tokenizer\n    \n    Args:\n        model_dir: Directory containing the saved model and tokenizer\n    \"\"\"\n    model_path = os.path.join(model_dir, \"model\")\n    tokenizer_path = os.path.join(model_dir, \"tokenizer\")\n    \n    if not os.path.exists(model_path) or not os.path.exists(tokenizer_path):\n        print(f\"Model or tokenizer not found in {model_dir}. Downloading...\")\n        return download_and_save_model(output_dir=model_dir)[0:2]\n    \n    print(f\"Loading model from {model_path}...\")\n    \n    # Configure quantization\n    compute_dtype = torch.float16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    )\n    \n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        tokenizer_path,\n        trust_remote_code=True,\n        padding_side=\"left\"\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    return model, tokenizer\n\n# Usage examples:\n# Option 1: Download, save and get the model and tokenizer objects\n# model, tokenizer, saved_dir = download_and_save_model()\n\n# Option 2: Just save the model for later use\n# _, _, saved_dir = download_and_save_model()\n\n# Option 3: Load a previously saved model\n# model, tokenizer = load_saved_model(\"saved_model\")\n\n# Execute the function\nmodel, tokenizer, saved_dir = download_and_save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:09:43.442249Z","iopub.execute_input":"2025-04-07T14:09:43.442616Z","iopub.status.idle":"2025-04-07T14:10:11.547766Z","shell.execute_reply.started":"2025-04-07T14:09:43.442588Z","shell.execute_reply":"2025-04-07T14:10:11.546286Z"}},"outputs":[{"name":"stdout","text":"Downloading model mistralai/Mistral-7B-Instruct-v0.2...\nLoading Mistral model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01b64e461ef4bbdae16351472a9eec2"}},"metadata":{}},{"name":"stdout","text":"Model saved to saved_model/model\nTokenizer saved to saved_model/tokenizer\nModel and tokenizer saved to saved_model\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import shutil\n\n# Zip the model directory\noutput_zip = 'modelsumz'\nshutil.make_archive(output_zip, 'zip', '/kaggle/working/saved_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:11:21.988204Z","iopub.execute_input":"2025-04-07T14:11:21.988560Z","iopub.status.idle":"2025-04-07T14:15:07.696673Z","shell.execute_reply.started":"2025-04-07T14:11:21.988524Z","shell.execute_reply":"2025-04-07T14:15:07.695819Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/modelsumz.zip'"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('/kaggle/working/modelsumz.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:15:07.697889Z","iopub.execute_input":"2025-04-07T14:15:07.698204Z","iopub.status.idle":"2025-04-07T14:15:07.703250Z","shell.execute_reply.started":"2025-04-07T14:15:07.698173Z","shell.execute_reply":"2025-04-07T14:15:07.702438Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/modelsumz.zip","text/html":"<a href='/kaggle/working/modelsumz.zip' target='_blank'>/kaggle/working/modelsumz.zip</a><br>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}