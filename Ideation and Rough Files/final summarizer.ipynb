{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11301183,"sourceType":"datasetVersion","datasetId":7067443},{"sourceId":11309777,"sourceType":"datasetVersion","datasetId":7073389},{"sourceId":11311208,"sourceType":"datasetVersion","datasetId":7074482}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:08:47.621593Z","iopub.execute_input":"2025-04-07T13:08:47.621856Z","iopub.status.idle":"2025-04-07T13:08:53.574254Z","shell.execute_reply.started":"2025-04-07T13:08:47.621825Z","shell.execute_reply":"2025-04-07T13:08:53.573224Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.2.1)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.3.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.47.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.11.12)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nDownloading trl-0.16.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.16.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:08:53.575209Z","iopub.execute_input":"2025-04-07T13:08:53.575456Z","iopub.status.idle":"2025-04-07T13:08:56.915678Z","shell.execute_reply.started":"2025-04-07T13:08:53.575434Z","shell.execute_reply":"2025-04-07T13:08:56.914617Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:08:56.916783Z","iopub.execute_input":"2025-04-07T13:08:56.917140Z","iopub.status.idle":"2025-04-07T13:09:00.319442Z","shell.execute_reply.started":"2025-04-07T13:08:56.917103Z","shell.execute_reply":"2025-04-07T13:09:00.318411Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:00.321551Z","iopub.execute_input":"2025-04-07T13:09:00.321812Z","iopub.status.idle":"2025-04-07T13:09:06.494270Z","shell.execute_reply.started":"2025-04-07T13:09:00.321789Z","shell.execute_reply":"2025-04-07T13:09:06.493274Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport torch\nimport re\nimport ast\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:06.495497Z","iopub.execute_input":"2025-04-07T13:09:06.495797Z","iopub.status.idle":"2025-04-07T13:09:17.306099Z","shell.execute_reply.started":"2025-04-07T13:09:06.495764Z","shell.execute_reply":"2025-04-07T13:09:17.305191Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Get HuggingFace token\ndef get_hf_token():\n    user_secrets = UserSecretsClient()\n    return user_secrets.get_secret(\"HF_TOKEN\")\n\ndef safe_eval(s):\n    \"\"\"Safely evaluate a string as a Python expression\"\"\"\n    if not isinstance(s, str):\n        return s\n    try:\n        return ast.literal_eval(s)\n    except (SyntaxError, ValueError):\n        return s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.309371Z","iopub.execute_input":"2025-04-07T13:09:17.309679Z","iopub.status.idle":"2025-04-07T13:09:17.315076Z","shell.execute_reply.started":"2025-04-07T13:09:17.309647Z","shell.execute_reply":"2025-04-07T13:09:17.314224Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_employee_data(file_path=\"/kaggle/input/distressed-employees/distressed_employees_new.csv\"):\n    \"\"\"Load and process employee data from CSV\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"File not found at {file_path}\")\n        \n        df = pd.read_csv(file_path)\n        \n        # Convert string representations of lists to actual lists\n        for col in ['Problems', 'Other Problems']:\n            if col in df.columns:\n                df[col] = df[col].apply(lambda x: safe_eval(x) if isinstance(x, str) else x)\n        \n        return df\n    except Exception as e:\n        print(f\"Error loading employee data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.317602Z","iopub.execute_input":"2025-04-07T13:09:17.317797Z","iopub.status.idle":"2025-04-07T13:09:17.352160Z","shell.execute_reply.started":"2025-04-07T13:09:17.317780Z","shell.execute_reply":"2025-04-07T13:09:17.351404Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def load_model_and_tokenizer():\n    \"\"\"Load the Mistral AI model and tokenizer with 4-bit quantization\"\"\"\n    print(\"Loading Mistral model...\")\n    hf_token = get_hf_token()\n    \n    compute_dtype = torch.float16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.2\",\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        token=hf_token\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.2\",\n        trust_remote_code=True,\n        padding_side=\"left\",\n        token=hf_token\n    )\n    \n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.353373Z","iopub.execute_input":"2025-04-07T13:09:17.353680Z","iopub.status.idle":"2025-04-07T13:09:17.371089Z","shell.execute_reply.started":"2025-04-07T13:09:17.353652Z","shell.execute_reply":"2025-04-07T13:09:17.370508Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def extract_conversations_from_qna(chat_history):\n    \"\"\"Extract and format conversations from chat history\"\"\"\n    if not chat_history:\n        return \"No chat history available\"\n        \n    if isinstance(chat_history, str):\n        # Check if it's a file path\n        if os.path.exists(chat_history):\n            try:\n                with open(chat_history, 'r') as f:\n                    chat_history = json.load(f)\n            except json.JSONDecodeError:\n                with open(chat_history, 'r') as f:\n                    return f.read()\n        else:\n            # Try to parse as JSON string\n            try:\n                chat_history = json.loads(chat_history)\n            except json.JSONDecodeError:\n                return chat_history\n\n    # Process based on structure\n    formatted_chat = \"\"\n    if isinstance(chat_history, list):\n        for entry in chat_history:\n            if isinstance(entry, dict):\n                # Handle the specific format with direction and message\n                if 'direction' in entry and 'message' in entry:\n                    direction = entry.get('direction', '')\n                    message = entry.get('message', '')\n                    if direction == 'sent':\n                        formatted_chat += f\"User: {message}\\n\\n\"\n                    elif direction == 'received':\n                        formatted_chat += f\"Bot: {message}\\n\\n\"\n                # Handle standard chat format with role and content\n                elif 'role' in entry and 'content' in entry:\n                    role = entry.get('role', '')\n                    content = entry.get('content', '')\n                    formatted_chat += f\"{role.capitalize()}: {content}\\n\\n\"\n                # Handle QnA format\n                elif 'question' in entry and 'answer' in entry:\n                    question = entry.get('question', '')\n                    answer = entry.get('answer', '')\n                    formatted_chat += f\"User: {question}\\nBot: {answer}\\n\\n\"\n                # Handle other formats\n                elif any(key in entry for key in ['user', 'bot', 'assistant', 'system']):\n                    for key, value in entry.items():\n                        if isinstance(value, str) and value.strip():\n                            formatted_chat += f\"{key.capitalize()}: {value}\\n\\n\"\n            elif isinstance(entry, str):\n                formatted_chat += f\"{entry}\\n\\n\"\n    elif isinstance(chat_history, dict):\n        # Handle different dictionary formats\n        if 'messages' in chat_history:\n            return extract_conversations_from_qna(chat_history['messages'])\n        elif 'history' in chat_history:\n            return extract_conversations_from_qna(chat_history['history'])\n        else:\n            for key, value in chat_history.items():\n                if isinstance(value, str) and value.strip():\n                    formatted_chat += f\"{key.capitalize()}: {value}\\n\\n\"\n    else:\n        formatted_chat = str(chat_history)\n    \n    return formatted_chat.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:34:31.521442Z","iopub.execute_input":"2025-04-07T13:34:31.521837Z","iopub.status.idle":"2025-04-07T13:34:31.530697Z","shell.execute_reply.started":"2025-04-07T13:34:31.521805Z","shell.execute_reply":"2025-04-07T13:34:31.529955Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def format_employee_data(employee_data):\n    \"\"\"Format employee data for the prompt\"\"\"\n    formatted_data = {}\n    \n    # Basic info\n    formatted_data[\"Employee ID\"] = employee_data.get(\"Employee_ID\", \"\")\n    formatted_data[\"Average Work Hours\"] = employee_data.get(\"Average Work Hours\", \"\")\n    formatted_data[\"Performance Rating\"] = employee_data.get(\"Performance Rating\", \"\")\n    formatted_data[\"Reward Factor\"] = employee_data.get(\"Reward Factor\", \"\")\n    formatted_data[\"Vibe Factor\"] = employee_data.get(\"Vibe Factor\", \"\")\n    formatted_data[\"Anomaly Score\"] = employee_data.get(\"Anamaly_Score\", \"\")\n    \n    # Problems\n    formatted_data[\"Problems\"] = []\n    problems = employee_data.get(\"Problems\", [])\n    if problems and isinstance(problems, list):\n        for problem in problems:\n            if isinstance(problem, list) and len(problem) >= 2:\n                formatted_data[\"Problems\"].append({\n                    \"issue\": problem[0],\n                    \"score\": problem[1]\n                })\n    \n    # Other Problems\n    formatted_data[\"Other Problems\"] = []\n    other_problems = employee_data.get(\"Other Problems\", [])\n    if other_problems and isinstance(other_problems, list):\n        for problem in other_problems:\n            if isinstance(problem, list) and len(problem) >= 2:\n                formatted_data[\"Other Problems\"].append({\n                    \"issue\": problem[0],\n                    \"score\": problem[1]\n                })\n    \n    return json.dumps(formatted_data, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.399887Z","iopub.execute_input":"2025-04-07T13:09:17.400077Z","iopub.status.idle":"2025-04-07T13:09:17.425866Z","shell.execute_reply.started":"2025-04-07T13:09:17.400059Z","shell.execute_reply":"2025-04-07T13:09:17.425217Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def generate_hr_report(model, tokenizer, employee_id, employee_data, chat_history):\n    \"\"\"Generate HR report using the Mistral model\"\"\"\n    processed_chat = extract_conversations_from_qna(chat_history)\n    formatted_data = format_employee_data(employee_data)\n    \n    # Create the prompt for Mistral format\n    prompt = f\"\"\"<s>[INST]\n    You are an AI assistant that analyzes conversations between employees and a chatbot, then creates comprehensive HR reports based on distressed employee data. Your task is to:\n    \n    1. Analyze conversation tone/content\n    2. Identify employee concerns\n    3. Extract engagement/satisfaction insights\n    4. Assess risks\n    5. Recommend actionable steps\n    \n    CRITICAL INSTRUCTION: NEVER use \"N/A\" or placeholder text anywhere in your report. If data appears missing, make reasonable inferences based on available information or provide generic but meaningful content.\n    \n    Create a STRUCTURED report with these REQUIRED sections:\n    \n    ## 💼 Employee Summary\n    • Basic Info:\n      - Employee ID: {employee_id}\n      - Problem indicators (from Problems and Other Problems columns)\n      - Work hours and performance metrics\n    • Quantitative Metrics:\n      - Anomaly Score interpretation\n      - Vibe Factor analysis\n      - Reward Factor evaluation\n      - Performance Rating context\n      - Average Work Hours assessment\n    • Qualitative Analysis:\n      - Top issues identified in Problems column\n      - Secondary issues from Other Problems column\n      - Risk patterns and correlations\n    \n    ## 🔍 Key Insights\n    • Technical Observations:\n      - Analyze highest-scoring problem factors \n      - Identify patterns across problem indicators\n    • Quantitative Analysis:\n      - Compare metrics to normal ranges\n      - Analyze correlation between Problems and numeric indicators\n    \n    ## 🚨 Risk Assessment\n    • Concerns:\n      - Low-level concerns (scores 0.1-0.3)\n      - Medium-level concerns (scores 0.3-0.6)\n      - High-level concerns (scores >0.6)\n    • Anomalies:\n      - Interpret Anomaly Score relative to company average\n      - Analyze unusual patterns in Problems and Other Problems\n    • Indicators:\n      - Connect conversation content with identified problem areas\n      - Highlight behavioral indicators from chat history\n    \n    ## 📈 Recommended Actions\n    • Critical Steps:\n      - 3-5 highest priority actions targeting top problem areas\n      - Specific responsibility assignments (HR/manager/employee)\n      - Clear timelines for implementation\n    • Additional Considerations:\n      - Follow-up procedures based on Anomaly Score\n      - Support resources for identified Problems\n      - Preventive measures for potential issues\n    \n    FORMATTING RULES:\n    ✅ Use EXACT section headers with emojis as shown above\n    ✅ Bold subsection headers using ** **\n    ✅ Each bullet point must contain 15+ meaningful words\n    ✅ Use • for main points and + for subpoints\n    ✅ Include ALL data points from employee information\n    \n    CRITICAL REQUIREMENTS:\n    ✅ NO PLACEHOLDERS - never use \"N/A,\" \"TBD,\" or similar text\n    ✅ Balanced section lengths\n    ✅ Complete action items addressing ALL risks\n    ✅ Specific timelines and responsibility assignments\n    \n    If you're running out of space, be more concise in earlier sections but NEVER use placeholder text or leave sections incomplete.\n    <</SYS>>\n    \n    Employee: {employee_id}\n    Data: {formatted_data}\n    Conversation: {processed_chat}\n    \n    Generate a COMPLETE HR report with NO placeholder text like \"N/A\". Every section must contain meaningful content based on the distressed employees dataset structure and chat history. [/INST]\n    \"\"\"\n    \n    # Generate the report\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    output = model.generate(\n        **inputs,\n        max_length=4096,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.15\n    )\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    # Extract just the response part (after the prompt)\n    response_parts = response.split(\"[/INST]\")\n    if len(response_parts) > 1:\n        response = response_parts[-1].strip()\n    else:\n        # If splitting failed, try to extract the report content\n        start_idx = response.find(\"## 💼 Employee Summary\")\n        if start_idx != -1:\n            response = response[start_idx:].strip()\n    \n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:43:12.650692Z","iopub.execute_input":"2025-04-07T13:43:12.651038Z","iopub.status.idle":"2025-04-07T13:43:12.658025Z","shell.execute_reply.started":"2025-04-07T13:43:12.651005Z","shell.execute_reply":"2025-04-07T13:43:12.657143Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def convert_markdown_to_structured_json(markdown_text):\n    \"\"\"Convert markdown HR report to structured JSON using line-by-line parsing\"\"\"\n    # Initialize the structure\n    report = {\n        \"Employee Summary\": {\n            \"Basic Info\": [],\n            \"Quantitative Metrics\": [],\n            \"Qualitative Metrics\": []\n        },\n        \"Key Insights\": {\n            \"Technical Observations\": [],\n            \"Quantitative Analysis\": []\n        },\n        \"Risk Assessment\": {\n            \"Concerns\": [],\n            \"Anomalies\": [],\n            \"Indicators\": []\n        },\n        \"Recommended Actions\": {\n            \"Critical Steps\": [],\n            \"Additional Considerations\": []\n        }\n    }\n    \n    # Clean the markdown text by removing indentation\n    cleaned_lines = [line.lstrip() for line in markdown_text.split('\\n')]\n    \n    current_section = None\n    current_subsection = None\n    \n    for line in cleaned_lines:\n        if not line:\n            continue\n            \n        # Check for main section headers\n        if line.startswith('## '):\n            section_title = line[3:].strip()\n            clean_title = re.sub(r'[^\\w\\s]', '', section_title).strip()\n            \n            for key in report.keys():\n                if clean_title.lower() in key.lower() or key.lower() in clean_title.lower():\n                    current_section = key\n                    current_subsection = None\n                    break\n                \n        # Check for subsection headers\n        elif line.startswith('### '):\n            if not current_section:\n                continue\n                \n            subsection_title = line[4:].strip()\n            if subsection_title.endswith(':'):\n                subsection_title = subsection_title[:-1]\n                \n            # Find matching subsection\n            for key in report[current_section].keys():\n                if subsection_title.lower() in key.lower() or key.lower() in subsection_title.lower():\n                    current_subsection = key\n                    break\n        \n        # Check for bullet points (handles different styles)\n        elif (line.startswith('-') or line.startswith('•') or line.startswith('+') or \n              line.startswith('1.') or line.startswith('2.') or line.startswith('3.') or \n              line.startswith('4.') or line.startswith('5.')):\n            \n            if not current_section or not current_subsection:\n                continue\n                \n            # Extract bullet text (everything after the bullet marker)\n            marker_end = line.find(' ')\n            if marker_end != -1:\n                bullet_text = line[marker_end+1:].strip()\n                if bullet_text and bullet_text != \"**\":\n                    # Handle cases where there might be bold formatting\n                    if bullet_text.startswith('**') and '**:' in bullet_text:\n                        # Skip subsection headers in bullet format\n                        continue\n                    report[current_section][current_subsection].append(bullet_text)\n    \n    # Add this code here, just before the return statement\n    # Remove empty arrays from the report\n    for section, subsections in report.items():\n        for subsection in list(subsections.keys()):\n            if not subsections[subsection]:\n                del subsections[subsection]\n    \n    return report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:02:02.580559Z","iopub.execute_input":"2025-04-07T14:02:02.580898Z","iopub.status.idle":"2025-04-07T14:02:02.590149Z","shell.execute_reply.started":"2025-04-07T14:02:02.580872Z","shell.execute_reply":"2025-04-07T14:02:02.589359Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def save_report_to_json(report_data, employee_id, output_dir=\"./reports\"):\n    \"\"\"Save the generated report to a JSON file\"\"\"\n    # Create output directory if it doesn't exist\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    \n    # Create filename\n    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = output_path / f\"hr_report_{employee_id}_{timestamp}.json\"\n    \n    # Save JSON\n    with open(filename, 'w') as f:\n        json.dump(report_data, f, indent=2)\n    \n    print(f\"Report saved to {filename}\")\n    return str(filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:17.476710Z","iopub.execute_input":"2025-04-07T13:09:17.476927Z","iopub.status.idle":"2025-04-07T13:09:17.497399Z","shell.execute_reply.started":"2025-04-07T13:09:17.476907Z","shell.execute_reply":"2025-04-07T13:09:17.496834Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def generate_hr_summary(employee_id, chat_history=None, output_dir=\"./reports\"):\n    \"\"\"End-to-end process to generate HR summary\"\"\"\n    print(f\"Generating HR summary for employee {employee_id}\")\n    \n    # Load employee data\n    file_path = \"/kaggle/input/distressed-employees/distressed_employees_new.csv\"\n    df = load_employee_data(file_path)\n    \n    if df is None:\n        return {\"error\": \"Failed to load employee data\"}\n    \n    # Filter data for the specified employee\n    employee_rows = df[df['Employee_ID'] == employee_id]\n    \n    if len(employee_rows) == 0:\n        return {\"error\": f\"Employee ID {employee_id} not found in the dataset\"}\n    \n    employee_data = employee_rows.iloc[0].to_dict()\n    \n    # Load model and tokenizer\n    model, tokenizer = load_model_and_tokenizer()\n    \n    # Generate report\n    markdown_report = generate_hr_report(model, tokenizer, employee_id, employee_data, chat_history)\n    \n    # Convert markdown to structured JSON\n    structured_report = convert_markdown_to_structured_json(markdown_report)\n    \n    # Prepare final report\n    final_report = {\n        \"employee_id\": employee_id,\n        \"report_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"report\": structured_report,\n        \"raw_markdown\": markdown_report\n    }\n    \n    # Save report\n    report_file = save_report_to_json(final_report, employee_id, output_dir)\n    \n    return {\n        \"status\": \"success\",\n        \"employee_id\": employee_id,\n        \"report_file\": report_file\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:09:18.005414Z","iopub.execute_input":"2025-04-07T13:09:18.005731Z","iopub.status.idle":"2025-04-07T13:09:18.011510Z","shell.execute_reply.started":"2025-04-07T13:09:18.005707Z","shell.execute_reply":"2025-04-07T13:09:18.010572Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Main execution function with command line arguments\ndef main():\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Generate HR summary reports')\n    parser.add_argument('--employee_id', type=str, required=True, help='Employee ID')\n    parser.add_argument('--chat_file', type=str, help='Path to chat history JSON file')\n    parser.add_argument('--output_dir', type=str, default='./reports', help='Output directory for reports')\n    \n    args = parser.parse_args()\n    \n    # Generate report\n    result = generate_hr_summary(args.employee_id, args.chat_file, args.output_dir)\n    print(json.dumps(result, indent=2))\n\n# Replace the main() function with this code\nif __name__ == \"__main__\":\n    # Get employee ID directly from input\n    employee_id = input(\"Enter Employee ID (e.g., EMP0418): \")\n    \n    # Optional: Get chat file path (or leave empty)\n    chat_file = input(\"Enter chat file path (optional, press Enter to skip): \") or None\n    \n    # Use default output directory or customize\n    output_dir = \"./reports\"\n    \n    # Generate report\n    result = generate_hr_summary(employee_id, chat_file, output_dir)\n    print(json.dumps(result, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:02:08.338285Z","iopub.execute_input":"2025-04-07T14:02:08.338597Z","iopub.status.idle":"2025-04-07T14:04:20.672733Z","shell.execute_reply.started":"2025-04-07T14:02:08.338570Z","shell.execute_reply":"2025-04-07T14:04:20.671961Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter Employee ID (e.g., EMP0418):  EMP0040\nEnter chat file path (optional, press Enter to skip):  /kaggle/input/chat-history/chat_history.json\n"},{"name":"stdout","text":"Generating HR summary for employee EMP0040\nLoading Mistral model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4560b1b8ac11409aa48f9b0d479808a5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Report saved to reports/hr_report_EMP0040_20250407_140420.json\n{\n  \"status\": \"success\",\n  \"employee_id\": \"EMP0040\",\n  \"report_file\": \"reports/hr_report_EMP0040_20250407_140420.json\"\n}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom kaggle_secrets import UserSecretsClient\n\ndef get_hf_token():\n    \"\"\"Get HuggingFace token from Kaggle secrets\"\"\"\n    user_secrets = UserSecretsClient()\n    return user_secrets.get_secret(\"HF_TOKEN\")\n\ndef download_and_save_model(model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", output_dir=\"saved_model\", use_token=True):\n    \"\"\"\n    Downloads a model from Hugging Face and saves it to a local directory\n    \n    Args:\n        model_name: HuggingFace model name\n        output_dir: Directory to save the model\n        use_token: Whether to use HF token from Kaggle secrets\n    \"\"\"\n    print(f\"Downloading model {model_name}...\")\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Get HF token if needed\n    hf_token = get_hf_token() if use_token else None\n    \n    # Configure quantization for memory efficiency\n    compute_dtype = torch.float16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    )\n    \n    # Download model\n    print(\"Loading Mistral model...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        token=hf_token\n    )\n    \n    # Save the model to local directory\n    model_path = os.path.join(output_dir, \"model\")\n    os.makedirs(model_path, exist_ok=True)\n    model.save_pretrained(model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    # Download and save tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        padding_side=\"left\",\n        token=hf_token\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer_path = os.path.join(output_dir, \"tokenizer\")\n    os.makedirs(tokenizer_path, exist_ok=True)\n    tokenizer.save_pretrained(tokenizer_path)\n    print(f\"Tokenizer saved to {tokenizer_path}\")\n    \n    print(f\"Model and tokenizer saved to {output_dir}\")\n    return model, tokenizer, output_dir\n\ndef load_saved_model(model_dir=\"saved_model\"):\n    \"\"\"\n    Loads a previously downloaded model and tokenizer\n    \n    Args:\n        model_dir: Directory containing the saved model and tokenizer\n    \"\"\"\n    model_path = os.path.join(model_dir, \"model\")\n    tokenizer_path = os.path.join(model_dir, \"tokenizer\")\n    \n    if not os.path.exists(model_path) or not os.path.exists(tokenizer_path):\n        print(f\"Model or tokenizer not found in {model_dir}. Downloading...\")\n        return download_and_save_model(output_dir=model_dir)[0:2]\n    \n    print(f\"Loading model from {model_path}...\")\n    \n    # Configure quantization\n    compute_dtype = torch.float16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    )\n    \n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        tokenizer_path,\n        trust_remote_code=True,\n        padding_side=\"left\"\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    return model, tokenizer\n\n# Usage examples:\n# Option 1: Download, save and get the model and tokenizer objects\n# model, tokenizer, saved_dir = download_and_save_model()\n\n# Option 2: Just save the model for later use\n# _, _, saved_dir = download_and_save_model()\n\n# Option 3: Load a previously saved model\n# model, tokenizer = load_saved_model(\"saved_model\")\n\n# Execute the function\nmodel, tokenizer, saved_dir = download_and_save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:09:43.442249Z","iopub.execute_input":"2025-04-07T14:09:43.442616Z","iopub.status.idle":"2025-04-07T14:10:11.547766Z","shell.execute_reply.started":"2025-04-07T14:09:43.442588Z","shell.execute_reply":"2025-04-07T14:10:11.546286Z"}},"outputs":[{"name":"stdout","text":"Downloading model mistralai/Mistral-7B-Instruct-v0.2...\nLoading Mistral model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01b64e461ef4bbdae16351472a9eec2"}},"metadata":{}},{"name":"stdout","text":"Model saved to saved_model/model\nTokenizer saved to saved_model/tokenizer\nModel and tokenizer saved to saved_model\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import shutil\n\n# Zip the model directory\noutput_zip = 'modelsumz'\nshutil.make_archive(output_zip, 'zip', '/kaggle/working/saved_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:11:21.988204Z","iopub.execute_input":"2025-04-07T14:11:21.988560Z","iopub.status.idle":"2025-04-07T14:15:07.696673Z","shell.execute_reply.started":"2025-04-07T14:11:21.988524Z","shell.execute_reply":"2025-04-07T14:15:07.695819Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/modelsumz.zip'"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('/kaggle/working/modelsumz.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:15:07.697889Z","iopub.execute_input":"2025-04-07T14:15:07.698204Z","iopub.status.idle":"2025-04-07T14:15:07.703250Z","shell.execute_reply.started":"2025-04-07T14:15:07.698173Z","shell.execute_reply":"2025-04-07T14:15:07.702438Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/modelsumz.zip","text/html":"<a href='/kaggle/working/modelsumz.zip' target='_blank'>/kaggle/working/modelsumz.zip</a><br>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}