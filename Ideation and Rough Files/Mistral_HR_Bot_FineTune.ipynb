{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd16 Fine-Tune Mistral-7B for Empathetic HR Bot using QLoRA\n", "This notebook demonstrates how to fine-tune [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) on a small HR-focused dialogue dataset using QLoRA with Hugging Face `peft` and `transformers`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Install required libraries\n", "!pip install -q accelerate bitsandbytes peft transformers datasets trl"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Sample training dataset (you can later upload your own below)\n", "from datasets import Dataset\n", "\n", "data = {\n", "    \"prompt\": [\n", "        \"Employee Problem: I've been feeling exhausted at work.\\nAssistant:\",\n", "        \"Employee Problem: I can't focus on my tasks lately.\\nAssistant:\",\n", "    ],\n", "    \"completion\": [\n", "        \"I'm really sorry to hear that. Can you share what might be causing this exhaustion? Have things changed recently?\",\n", "        \"That must be tough. Is there anything that's been distracting or worrying you outside or inside of work?\",\n", "    ]\n", "}\n", "\n", "dataset = Dataset.from_dict(data)\n", "dataset = dataset.train_test_split(test_size=0.2)\n", "\n", "dataset[\"train\"].to_json(\"train_data.json\", lines=True)\n", "dataset[\"test\"].to_json(\"eval_data.json\", lines=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2b06\ufe0f Optionally upload your own dataset (JSON format)\n", "from google.colab import files\n", "uploaded = files.upload()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udde0 Define and prepare model (Mistral-7B-Instruct)\n", "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n", "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n", "import torch\n", "\n", "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n", "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    load_in_8bit=True,\n", "    device_map=\"auto\",\n", "    trust_remote_code=True\n", ")\n", "model = prepare_model_for_kbit_training(model)\n", "\n", "peft_config = LoraConfig(\n", "    r=8,\n", "    lora_alpha=16,\n", "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n", "    lora_dropout=0.05,\n", "    bias=\"none\",\n", "    task_type=\"CAUSAL_LM\"\n", ")\n", "model = get_peft_model(model, peft_config)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\uddfe Format dataset with system prompt\n", "SYSTEM_PROMPT = '''<|system|>\n", "You are a supportive HR assistant. When an employee shares a problem, your goal is to gently ask about potential causes with empathy. Once you gather enough info, suggest a helpful solution if appropriate.\n", "</s>'''\n", "\n", "def format_prompt(example):\n", "    return {\n", "        \"input_ids\": tokenizer(SYSTEM_PROMPT + example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\").input_ids[0],\n", "        \"labels\": tokenizer(example[\"completion\"], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").input_ids[0],\n", "    }\n", "\n", "tokenized_dataset = dataset.map(format_prompt)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udd27 Training configuration\n", "training_args = TrainingArguments(\n", "    output_dir=\"./results\",\n", "    per_device_train_batch_size=2,\n", "    per_device_eval_batch_size=2,\n", "    num_train_epochs=3,\n", "    evaluation_strategy=\"epoch\",\n", "    save_strategy=\"epoch\",\n", "    learning_rate=2e-4,\n", "    bf16=False,\n", "    fp16=True,\n", "    logging_steps=10,\n", "    report_to=\"none\"\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_dataset[\"train\"],\n", "    eval_dataset=tokenized_dataset[\"test\"],\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\ude80 Start training\n", "trainer.train()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udcac Test the fine-tuned model\n", "def chat_with_bot(employee_problem):\n", "    prompt = SYSTEM_PROMPT + f\"Employee Problem: {employee_problem}\\nAssistant:\"\n", "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n", "    output = model.generate(**inputs, max_new_tokens=150)\n", "    print(tokenizer.decode(output[0], skip_special_tokens=True))\n", "\n", "# Example usage:\n", "chat_with_bot(\"I feel disconnected from my team lately.\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}