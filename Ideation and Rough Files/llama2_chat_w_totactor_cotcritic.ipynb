{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11297877,"sourceType":"datasetVersion","datasetId":7064870},{"sourceId":324476,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":273171,"modelId":294118}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:23:26.433134Z","iopub.execute_input":"2025-04-07T04:23:26.433422Z","iopub.status.idle":"2025-04-07T04:23:31.753753Z","shell.execute_reply.started":"2025-04-07T04:23:26.433392Z","shell.execute_reply":"2025-04-07T04:23:31.752689Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.2.1)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.3.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.47.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.11.12)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nDownloading trl-0.16.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.16.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:23:31.754667Z","iopub.execute_input":"2025-04-07T04:23:31.754911Z","iopub.status.idle":"2025-04-07T04:23:35.108499Z","shell.execute_reply.started":"2025-04-07T04:23:31.754891Z","shell.execute_reply":"2025-04-07T04:23:35.107667Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:23:35.109364Z","iopub.execute_input":"2025-04-07T04:23:35.109690Z","iopub.status.idle":"2025-04-07T04:23:38.528233Z","shell.execute_reply.started":"2025-04-07T04:23:35.109659Z","shell.execute_reply":"2025-04-07T04:23:38.527373Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:23:38.530204Z","iopub.execute_input":"2025-04-07T04:23:38.530548Z","iopub.status.idle":"2025-04-07T04:23:44.811147Z","shell.execute_reply.started":"2025-04-07T04:23:38.530516Z","shell.execute_reply":"2025-04-07T04:23:44.810265Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:23:44.812469Z","iopub.execute_input":"2025-04-07T04:23:44.812693Z","iopub.status.idle":"2025-04-07T04:24:09.473361Z","shell.execute_reply.started":"2025-04-07T04:23:44.812674Z","shell.execute_reply":"2025-04-07T04:24:09.472726Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n# Load the dataset and drop Contributors column\nqna_db = pd.read_excel(\"/kaggle/input/final-qna-dataset/QnA_Sheet_Data_Final.xlsx\")\nqna_db.drop(['Contributors'], axis=1, inplace=True)\n\n# Forward fill the Employee_ID column to propagate values to NaN cells\nqna_db['Employee_ID'] = qna_db['Employee_ID'].fillna(method='ffill')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:09.474128Z","iopub.execute_input":"2025-04-07T04:24:09.474358Z","iopub.status.idle":"2025-04-07T04:24:09.985333Z","shell.execute_reply.started":"2025-04-07T04:24:09.474338Z","shell.execute_reply":"2025-04-07T04:24:09.984480Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-952d34758e24>:7: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  qna_db['Employee_ID'] = qna_db['Employee_ID'].fillna(method='ffill')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Verify the changes worked\nprint(qna_db.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:09.986091Z","iopub.execute_input":"2025-04-07T04:24:09.986724Z","iopub.status.idle":"2025-04-07T04:24:09.996327Z","shell.execute_reply.started":"2025-04-07T04:24:09.986700Z","shell.execute_reply":"2025-04-07T04:24:09.995623Z"}},"outputs":[{"name":"stdout","text":"  Employee_ID                                       Key Concerns  \\\n0     EMP0009  High recent casual leave (9.02), possibly affe...   \n1     EMP0009  Low meeting attendance (2 meetings, below data...   \n2     EMP0009  Moderate work activity (44 Teams messages, 50 ...   \n3     EMP0009  Slightly high work hours (16.28, highest in da...   \n4     EMP0009  Promotion consideration, but activity could be...   \n5     EMP0012  Low engagement in all activities (Teams: 12, E...   \n6     EMP0012  Moderate casual leave (8.51), could be affecti...   \n7     EMP0012           Needs improvement in performance rating.   \n8     EMP0012  Decayed Reward Points (57.14) are below averag...   \n9     EMP0012  Being considered for promotion despite perform...   \n\n                                            Question  \\\n0  You recently took casual leave. Did it impact ...   \n1  Your meeting attendance is lower than average....   \n2  Your activity levels in Teams and Emails are m...   \n3  Your work hours are the highest in this group....   \n4  Since you are being considered for a promotion...   \n5  Your engagement in Teams and Emails is quite l...   \n6  You recently took casual leave. Did it affect ...   \n7  Your performance rating suggests there’s room ...   \n8  Would additional training or mentorship help y...   \n9  You are being considered for a promotion. Do y...   \n\n                                        Response Set  \n0  <s>[INST] No, I planned my work ahead, so ever...  \n1  <s>[INST] I find most meetings unnecessary. [/...  \n2  <s>[INST] No, I feel comfortable with the curr...  \n3  <s>[INST] No, I feel comfortable with the curr...  \n4  <s>[INST] I need to work on my leadership skil...  \n5  <s>[INST] I prefer direct calls instead of mes...  \n6  <s>[INST] No, I planned my work ahead, so I di...  \n7  <s>[INST] Yes, my manager provides helpful gui...  \n8  <s>[INST] Yes, I think mentorship would help m...  \n9  <s>[INST] Yes, I feel confident and ready for ...  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"qna_db.dropna(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:09.997235Z","iopub.execute_input":"2025-04-07T04:24:09.997528Z","iopub.status.idle":"2025-04-07T04:24:10.009899Z","shell.execute_reply.started":"2025-04-07T04:24:09.997500Z","shell.execute_reply":"2025-04-07T04:24:10.009261Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import re\ndef extract_prompts(response_text):\n    \"\"\"\n    Extracts the user prompt and model answer from the response text.\n    Expects the format: <s>[INST] User prompt [/INST] Model answer</s>\n    \"\"\"\n    pattern = r\"<s>\\[INST\\]\\s*(.*?)\\s*\\[/INST\\]\\s*(.*?)\\s*</s>\"\n    match = re.search(pattern, response_text, re.DOTALL)\n    if match:\n        user_prompt = match.group(1)\n        model_answer = match.group(2)\n        return user_prompt, model_answer\n    else:\n        return None, None\n        \ndef create_new_format(row):\n    system_prompt = row['Question']\n    response_text = row['Response Set']\n    user_prompt, model_answer = extract_prompts(response_text)\n    \n    # If extraction fails, use the original response_text\n    if user_prompt is None or model_answer is None:\n        return response_text\n    \n    # Construct the new string format\n    new_string = (f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\"\n                  f\"{user_prompt} [/INST] {model_answer}</s>\")\n    return new_string","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.010727Z","iopub.execute_input":"2025-04-07T04:24:10.011018Z","iopub.status.idle":"2025-04-07T04:24:10.025283Z","shell.execute_reply.started":"2025-04-07T04:24:10.010994Z","shell.execute_reply":"2025-04-07T04:24:10.024586Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"qna_db['New Format'] = qna_db.apply(create_new_format, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.025898Z","iopub.execute_input":"2025-04-07T04:24:10.026118Z","iopub.status.idle":"2025-04-07T04:24:10.043979Z","shell.execute_reply.started":"2025-04-07T04:24:10.026099Z","shell.execute_reply":"2025-04-07T04:24:10.043362Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"new_db=qna_db['New Format'].to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.047465Z","iopub.execute_input":"2025-04-07T04:24:10.047658Z","iopub.status.idle":"2025-04-07T04:24:10.059182Z","shell.execute_reply.started":"2025-04-07T04:24:10.047642Z","shell.execute_reply":"2025-04-07T04:24:10.058420Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"new_db[5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.059995Z","iopub.execute_input":"2025-04-07T04:24:10.060262Z","iopub.status.idle":"2025-04-07T04:24:10.078323Z","shell.execute_reply.started":"2025-04-07T04:24:10.060235Z","shell.execute_reply":"2025-04-07T04:24:10.077516Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"\"<s>[INST] <<SYS>>\\nYour engagement in Teams and Emails is quite low. Are there any challenges preventing you from participating more?\\n<</SYS>>\\nI prefer direct calls instead of messages. [/INST] That's a valid approach! Would you like an option to log direct calls as collaboration activity?</s>\""},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.079095Z","iopub.execute_input":"2025-04-07T04:24:10.079343Z","iopub.status.idle":"2025-04-07T04:24:10.091543Z","shell.execute_reply.started":"2025-04-07T04:24:10.079324Z","shell.execute_reply":"2025-04-07T04:24:10.090783Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import re\nimport pandas as pd\n\n# Function to extract all Q&A pairs from Response Set\ndef extract_all_prompts(response_text):\n    \"\"\"\n    Extracts all user prompts and model answers from the response text.\n    \"\"\"\n    pattern = r\"<s>\\[INST\\]\\s*(.*?)\\s*\\[/INST\\]\\s*(.*?)\\s*</s>\"\n    matches = re.findall(pattern, response_text, re.DOTALL)\n    \n    # Return list of (user_prompt, model_answer) tuples\n    return matches if matches else []\n\n# Function to create expanded dataset with all Q&A pairs\ndef create_expanded_dataset(qna_db):\n    expanded_rows = []\n    employee_ids = []\n    key_concerns_list = []\n    \n    for _, row in qna_db.iterrows():\n        system_prompt = row['Question']\n        response_text = row['Response Set']\n        employee_id = row.get('Employee_ID', None)\n        key_concerns = row.get('Key Concerns', '')\n        \n        # Extract all Q&A pairs\n        qa_pairs = extract_all_prompts(response_text)\n        \n        # If no pairs found, skip or handle as needed\n        if not qa_pairs:\n            continue\n            \n        # Create a new row for each Q&A pair\n        for user_prompt, model_answer in qa_pairs:\n            new_string = (f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\"\n                        f\"{user_prompt} [/INST] {model_answer}</s>\")\n            expanded_rows.append(new_string)\n            employee_ids.append(employee_id)\n            key_concerns_list.append(key_concerns)\n    \n    # Create new DataFrame with all expanded rows and preserved columns\n    return pd.DataFrame({\n        'text': expanded_rows,\n        'Employee_ID': employee_ids,\n        'Key Concerns': key_concerns_list\n    })\n\n# Replace your existing transformation with this\nnew_db_pd = create_expanded_dataset(qna_db)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.092214Z","iopub.execute_input":"2025-04-07T04:24:10.092400Z","iopub.status.idle":"2025-04-07T04:24:10.123369Z","shell.execute_reply.started":"2025-04-07T04:24:10.092384Z","shell.execute_reply":"2025-04-07T04:24:10.122780Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Set display options to show the full content of columns\nimport pandas as pd\n\n# Option 1: Remove truncation for all columns\npd.set_option('display.max_colwidth', None)\n\n# Then view your data again\nnew_db_pd.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.124072Z","iopub.execute_input":"2025-04-07T04:24:10.124288Z","iopub.status.idle":"2025-04-07T04:24:10.149142Z","shell.execute_reply.started":"2025-04-07T04:24:10.124270Z","shell.execute_reply":"2025-04-07T04:24:10.148379Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                                                                                                                                                            text  \\\n0                                      <s>[INST] <<SYS>>\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n<</SYS>>\\nNo, I planned my work ahead, so everything was smooth. [/INST] That's great! Do you feel that planning ahead reduces stress at work?</s>   \n1      <s>[INST] <<SYS>>\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n<</SYS>>\\nYes, I had trouble catching up with my tasks after returning. [/INST] I see. Would having a buffer day to settle back in help you manage post-leave workload better?</s>   \n2                  <s>[INST] <<SYS>>\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n<</SYS>>\\nNot really, but I missed an important team discussion. [/INST] Understood. Would you find it helpful if key meeting summaries were shared automatically?</s>   \n3            <s>[INST] <<SYS>>\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n<</SYS>>\\nI was sick, so it was hard to manage pending tasks after my leave. [/INST] That makes sense. Do you think a flexible return schedule would help in such cases?</s>   \n4  <s>[INST] <<SYS>>\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n<</SYS>>\\nI haven't taken leave recently, maybe the data is incorrect. [/INST] Thanks for pointing that out! I'll check the records. Would you like to manually log recent leaves?</s>   \n\n  Employee_ID  \\\n0     EMP0009   \n1     EMP0009   \n2     EMP0009   \n3     EMP0009   \n4     EMP0009   \n\n                                                           Key Concerns  \n0  High recent casual leave (9.02), possibly affecting work engagement.  \n1  High recent casual leave (9.02), possibly affecting work engagement.  \n2  High recent casual leave (9.02), possibly affecting work engagement.  \n3  High recent casual leave (9.02), possibly affecting work engagement.  \n4  High recent casual leave (9.02), possibly affecting work engagement.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Employee_ID</th>\n      <th>Key Concerns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n&lt;&lt;/SYS&gt;&gt;\\nNo, I planned my work ahead, so everything was smooth. [/INST] That's great! Do you feel that planning ahead reduces stress at work?&lt;/s&gt;</td>\n      <td>EMP0009</td>\n      <td>High recent casual leave (9.02), possibly affecting work engagement.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n&lt;&lt;/SYS&gt;&gt;\\nYes, I had trouble catching up with my tasks after returning. [/INST] I see. Would having a buffer day to settle back in help you manage post-leave workload better?&lt;/s&gt;</td>\n      <td>EMP0009</td>\n      <td>High recent casual leave (9.02), possibly affecting work engagement.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n&lt;&lt;/SYS&gt;&gt;\\nNot really, but I missed an important team discussion. [/INST] Understood. Would you find it helpful if key meeting summaries were shared automatically?&lt;/s&gt;</td>\n      <td>EMP0009</td>\n      <td>High recent casual leave (9.02), possibly affecting work engagement.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n&lt;&lt;/SYS&gt;&gt;\\nI was sick, so it was hard to manage pending tasks after my leave. [/INST] That makes sense. Do you think a flexible return schedule would help in such cases?&lt;/s&gt;</td>\n      <td>EMP0009</td>\n      <td>High recent casual leave (9.02), possibly affecting work engagement.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n&lt;&lt;/SYS&gt;&gt;\\nI haven't taken leave recently, maybe the data is incorrect. [/INST] Thanks for pointing that out! I'll check the records. Would you like to manually log recent leaves?&lt;/s&gt;</td>\n      <td>EMP0009</td>\n      <td>High recent casual leave (9.02), possibly affecting work engagement.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Rename column if needed\n#transformed_dataset = transformed_dataset.rename(columns={transformed_dataset.columns[0]: \"text\"})\ntransformed_dataset = new_db_pd\ndataset = Dataset.from_pandas(transformed_dataset)\n# transformed_dataset = transformed_dataset.drop(columns=[\"Transformed_Train\"])\n# Check first sample\nprint(dataset[0])  # Expected: {'text': '### Human: My PIN is five thousand six. ### ...'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:10.149999Z","iopub.execute_input":"2025-04-07T04:24:10.150306Z","iopub.status.idle":"2025-04-07T04:24:10.173692Z","shell.execute_reply.started":"2025-04-07T04:24:10.150275Z","shell.execute_reply":"2025-04-07T04:24:10.173088Z"}},"outputs":[{"name":"stdout","text":"{'text': \"<s>[INST] <<SYS>>\\nYou recently took casual leave. Did it impact your workflow or team communication?\\n<</SYS>>\\nNo, I planned my work ahead, so everything was smooth. [/INST] That's great! Do you feel that planning ahead reduces stress at work?</s>\", 'Employee_ID': 'EMP0009', 'Key Concerns': 'High recent casual leave (9.02), possibly affecting work engagement.'}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def prepare_system_prompt(base_prompt, key_concerns=None, employee_info=None):\n    \"\"\"\n    Enriches the system prompt with employee context and key concerns\n    \"\"\"\n    enhanced_prompt = base_prompt\n    \n    if employee_info:\n        enhanced_prompt += f\"\\n\\nEmployee Information: {employee_info}\"\n    \n    if key_concerns:\n        enhanced_prompt += f\"\\n\\nKey Concerns: {key_concerns}\"\n        \n    return enhanced_prompt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:12.808559Z","iopub.execute_input":"2025-04-07T04:24:12.808847Z","iopub.status.idle":"2025-04-07T04:24:12.812806Z","shell.execute_reply.started":"2025-04-07T04:24:12.808828Z","shell.execute_reply":"2025-04-07T04:24:12.812042Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Advanced HR Chatbot with multiple prompting techniques\nclass AdvancedHRChatbot:\n    def __init__(self, model, tokenizer, base_system_prompt):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.base_system_prompt = base_system_prompt\n        self.chat_history = []\n        self.employee_contexts = {}\n        \n    def add_employee_context(self, employee_id, info, key_concerns):\n        \"\"\"Store employee context for in-context learning\"\"\"\n        self.employee_contexts[employee_id] = {\n            \"info\": info,\n            \"key_concerns\": key_concerns\n        }\n    \n    def _format_chat_history(self, max_history=5):\n        \"\"\"Format recent chat history for context\"\"\"\n        if not self.chat_history:\n            return \"\"\n        \n        recent_history = self.chat_history[-max_history:]\n        formatted = \"\\n\\nRecent Conversation:\\n\"\n        for entry in recent_history:\n            formatted += f\"Human: {entry['user']}\\nAI: {entry['assistant']}\\n\"\n        return formatted\n    \n    def _tree_of_thought_prompt(self, user_input, employee_id):\n        \"\"\"Generate a tree-of-thought style prompt\"\"\"\n        employee_ctx = self.employee_contexts.get(employee_id, {})\n        \n        # Chain-of-thought components\n        system_prompt = prepare_system_prompt(\n            self.base_system_prompt,\n            employee_ctx.get(\"key_concerns\", \"\"),\n            employee_ctx.get(\"info\", \"\")\n        )\n        \n        # Add chat history context\n        system_prompt += self._format_chat_history()\n        \n        # Tree-of-thought structure with multiple reasoning paths\n        tot_prompt = f\"\"\"\n{system_prompt}\n\nNow, I'll think through this question step by step:\n\nFirst perspective - Empathetic understanding:\n- What might the employee be feeling?\n- What emotional needs should I address?\n\nSecond perspective - Policy guidance:\n- What HR policies are relevant here?\n- What factual information should I provide?\n\nThird perspective - Root cause analysis:\n- What might be underlying issues?\n- How might I gently explore deeper concerns?\n\nBased on these thought paths, I'll formulate my response to: \"{user_input}\"\n\"\"\"\n        \n        return tot_prompt\n    \n    def _actor_critic_format(self, user_input, employee_id):\n        \"\"\"Actor-critic approach where model critiques its own responses\"\"\"\n        tot_prompt = self._tree_of_thought_prompt(user_input, employee_id)\n        \n        actor_critic_prompt = f\"\"\"\n{tot_prompt}\n\nACTOR RESPONSE:\n[Generate empathetic, helpful response]\n\nCRITIC EVALUATION:\n- Is this response empathetic enough?\n- Does it address potential underlying concerns?\n- Does it follow HR policy guidelines?\n- Is the tone appropriate for the employee's situation?\n\nFINAL IMPROVED RESPONSE:\n\"\"\"\n        return actor_critic_prompt\n    \n    def chat(self, user_input, employee_id=None, advanced_mode=\"tree\"):\n        \"\"\"\n        Process a user message and generate a response\n        advanced_mode options: \"basic\", \"chain\", \"tree\", \"actor_critic\"\n        \"\"\"\n        # Select prompting technique based on mode\n        if advanced_mode == \"basic\":\n            system_prompt = prepare_system_prompt(\n                self.base_system_prompt,\n                self.employee_contexts.get(employee_id, {}).get(\"key_concerns\", \"\"),\n                self.employee_contexts.get(employee_id, {}).get(\"info\", \"\")\n            )\n        elif advanced_mode == \"chain\":\n            system_prompt = prepare_system_prompt(\n                self.base_system_prompt,\n                self.employee_contexts.get(employee_id, {}).get(\"key_concerns\", \"\"),\n                self.employee_contexts.get(employee_id, {}).get(\"info\", \"\")\n            ) + self._format_chat_history()\n        elif advanced_mode == \"tree\":\n            system_prompt = self._tree_of_thought_prompt(user_input, employee_id)\n        elif advanced_mode == \"actor_critic\":\n            system_prompt = self._actor_critic_format(user_input, employee_id)\n        else:\n            system_prompt = self.base_system_prompt\n            \n        # Format according to Llama 2 template\n        prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_input} [/INST]\"\n        \n        # Generate response\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=512,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True,\n                repetition_penalty=1.2\n            )\n        \n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Extract just the model's reply (after [/INST])\n        try:\n            response = response.split(\"[/INST]\")[1].strip()\n        except IndexError:\n            response = response.split(user_input)[-1].strip()\n        \n        # Update chat history\n        self.chat_history.append({\n            \"user\": user_input,\n            \"assistant\": response\n        })\n        \n        return response\n        \n    def chat_loop(self, employee_id=None, advanced_mode=\"actor_critic\"):\n        \"\"\"Interactive chat loop similar to the Mistral example\"\"\"\n        print(f\"HR Assistant (using {advanced_mode} mode - type 'exit' to stop)\")\n        \n        while True:\n            user_input = input(\"Employee: \")\n            if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"thank you\", \"done\"]:\n                print(\"HR Assistant: Thank you for chatting with me. Have a great day!\")\n                break\n                \n            response = self.chat(user_input, employee_id, advanced_mode)\n            print(f\"HR Assistant: {response}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:13.628686Z","iopub.execute_input":"2025-04-07T04:24:13.628994Z","iopub.status.idle":"2025-04-07T04:24:13.640673Z","shell.execute_reply.started":"2025-04-07T04:24:13.628970Z","shell.execute_reply":"2025-04-07T04:24:13.639882Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:24:18.581019Z","iopub.execute_input":"2025-04-07T04:24:18.581317Z","iopub.status.idle":"2025-04-07T04:24:22.404204Z","shell.execute_reply.started":"2025-04-07T04:24:18.581297Z","shell.execute_reply":"2025-04-07T04:24:22.403096Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.4)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def main():\n    try:\n        import torch\n        \n        # Check GPU availability\n        if torch.cuda.is_available():\n            print(\"GPU is available. Using optimized configuration.\")\n            \n            # Original GPU configuration with BitsAndBytes\n            from transformers import BitsAndBytesConfig\n            \n            # Prepare quantization config\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.float16,\n                llm_int8_enable_fp32_cpu_offload=True\n            )\n            \n            # Load model with GPU configuration\n            model = AutoModelForCausalLM.from_pretrained(\n                \"/kaggle/input/llama-2-7b-chat-hf/pytorch/default/1\",\n                quantization_config=quantization_config,\n                device_map=\"auto\",\n                torch_dtype=torch.float16,\n                trust_remote_code=True\n            )\n        else:\n            print(\"GPU not available. Loading model in CPU mode...\")\n            \n            # CPU-compatible configuration without quantization\n            model = AutoModelForCausalLM.from_pretrained(\n                \"/kaggle/input/llama-2-7b-chat-hf/pytorch/default/1\",\n                device_map=\"cpu\",\n                low_cpu_mem_usage=True,\n                trust_remote_code=True\n            )\n \n        # Load tokenizer (same for both GPU and CPU)\n        tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-2-7b-chat-hf/pytorch/default/1\")\n\n        # Enhanced system prompt that combines elements from provided suggestions\n        BASE_SYSTEM_PROMPT = (\n            \"You are an empathetic, friendly HR chatbot named Elara who speaks gently and politely. \"\n            \"Your role is to slowly uncover the root cause of an employee's issue without being forceful or judgmental. \"\n            \"Begin with light conversation, ask how they are, and slowly ease into the topic. \"\n            \"Try to proceed with the conversation by understanding their mood before diving into problems one by one. \"\n            \"Maintain empathy throughout the conversation, provide guidance according to company policies, \"\n            \"and help employees resolve their concerns while being supportive and understanding.\"\n        )\n        \n        # Create the chatbot\n        hr_chatbot = AdvancedHRChatbot(model, tokenizer, BASE_SYSTEM_PROMPT)\n        \n        # Forward-fill Employee_ID values to handle NaN values\n        if 'Employee_ID' in qna_db.columns:\n            qna_db['Employee_ID'] = qna_db['Employee_ID'].fillna(method='ffill')\n        \n        # Automatically add employee contexts from your dataset\n        if 'Employee_ID' in qna_db.columns and 'Key Concerns' in qna_db.columns:\n            # Group by Employee_ID to get unique employees\n            for employee_id, group in qna_db.groupby('Employee_ID'):\n                # Extract employee information (handle potential NaN values)\n                key_concerns = group['Key Concerns'].iloc[0] if not pd.isna(group['Key Concerns'].iloc[0]) else \"\"\n                \n                # Basic employee info\n                info = f\"Employee ID: {employee_id}\"\n                \n                # Add this employee's context to the chatbot\n                hr_chatbot.add_employee_context(\n                    employee_id=employee_id,\n                    info=info,\n                    key_concerns=key_concerns\n                )\n            \n            print(f\"Added context for {len(qna_db['Employee_ID'].unique())} employees\")\n        else:\n            print(\"Required columns 'Employee_ID' or 'Key Concerns' not found in dataset\")\n            print(\"Using generalized HR assistant without employee-specific context\")\n            # Add a generic employee ID with no specific details\n            hr_chatbot.add_employee_context(\n                employee_id=\"GENERAL\",\n                info=\"\",\n                key_concerns=\"\"\n            )\n        \n        # Start interactive chat\n        available_employees = list(hr_chatbot.employee_contexts.keys()) \n        print(f\"Available employees: {available_employees}\")\n        \n        if available_employees:\n            # Display a numbered list of employees\n            for i, emp_id in enumerate(available_employees):\n                print(f\"{i+1}. {emp_id}\")\n            \n            # Get user selection\n            while True:\n                try:\n                    selection = int(input(\"Select an employee number (or 0 for general chatbot): \"))\n                    if selection == 0:\n                        selected_employee = \"GENERAL\"\n                        break\n                    elif 1 <= selection <= len(available_employees):\n                        selected_employee = available_employees[selection-1]\n                        break\n                    else:\n                        print(\"Invalid selection. Please try again.\")\n                except ValueError:\n                    print(\"Please enter a number.\")\n        else:\n            selected_employee = \"GENERAL\"\n        \n        print(f\"Starting chat with employee: {selected_employee}\")\n\n        hr_chatbot.chat_loop(employee_id=selected_employee, advanced_mode=\"actor_critic\")\n    \n    except Exception as e:\n        print(f\"Error initializing chatbot: {e}\")\n        import traceback\n        traceback.print_exc()  # Print detailed error information\n        print(\"Please check if the model path and data are correct.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T04:40:38.699759Z","iopub.execute_input":"2025-04-07T04:40:38.700289Z","iopub.status.idle":"2025-04-07T04:45:40.071476Z","shell.execute_reply.started":"2025-04-07T04:40:38.700249Z","shell.execute_reply":"2025-04-07T04:45:40.070705Z"}},"outputs":[{"name":"stdout","text":"GPU is available. Using optimized configuration.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9143dc2e1b33427fa5cd43cf1e1abf25"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-22-e6fbe19e5414>:58: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  qna_db['Employee_ID'] = qna_db['Employee_ID'].fillna(method='ffill')\n","output_type":"stream"},{"name":"stdout","text":"Added context for 30 employees\nAvailable employees: ['EMP0009', 'EMP0012', 'EMP0040', 'EMP0056', 'EMP0071', 'EMP0108', 'EMP0130', 'EMP0133', 'EMP0136', 'EMP0173', 'EMP0184', 'EMP0194', 'EMP0202', 'EMP0228', 'EMP0262', 'EMP0285', 'EMP0338', 'EMP0345', 'EMP0350', 'EMP0355', 'EMP0356', 'EMP0362', 'EMP0377', 'EMP0384', 'EMP0418', 'EMP0445', 'EMP0447', 'EMP0458', 'EMP0463', 'EMP0479']\n1. EMP0009\n2. EMP0012\n3. EMP0040\n4. EMP0056\n5. EMP0071\n6. EMP0108\n7. EMP0130\n8. EMP0133\n9. EMP0136\n10. EMP0173\n11. EMP0184\n12. EMP0194\n13. EMP0202\n14. EMP0228\n15. EMP0262\n16. EMP0285\n17. EMP0338\n18. EMP0345\n19. EMP0350\n20. EMP0355\n21. EMP0356\n22. EMP0362\n23. EMP0377\n24. EMP0384\n25. EMP0418\n26. EMP0445\n27. EMP0447\n28. EMP0458\n29. EMP0463\n30. EMP0479\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Select an employee number (or 0 for general chatbot):  EMP0262\n"},{"name":"stdout","text":"Please enter a number.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Select an employee number (or 0 for general chatbot):  17\n"},{"name":"stdout","text":"Starting chat with employee: EMP0338\nHR Assistant (using actor_critic mode - type 'exit' to stop)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Hi\n"},{"name":"stdout","text":"HR Assistant: Good afternoon! Would you like to log in automatically during future interactions?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Yeah\n"},{"name":"stdout","text":"HR Assistant: That’s good insight! Would more flexibility improve your balance?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  I think so\n"},{"name":"stdout","text":"HR Assistant: Understood! Would additional training or resources make the process smoother?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  I dont understand what you are saying\n"},{"name":"stdout","text":"HR Assistant: No problem! Would a call or email discussion clarify things further?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Yeah\n"},{"name":"stdout","text":"HR Assistant: Great idea! Would setting task deadlines improve productivity?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Yeah but the manager i have is stupid\n"},{"name":"stdout","text":"HR Assistant: Thanks for sharing that perspective! Would a different management style make work easier?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Remove the manager\n"},{"name":"stdout","text":"HR Assistant: Could leadership changes align better team dynamics?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Yeah\n"},{"name":"stdout","text":"HR Assistant: That makes sense! Would clearer priorities or tasks help you stay focused?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Fuck off\n"},{"name":"stdout","text":"HR Assistant: I see. Are there any specific challenges affecting your experience?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  I am not interested in this shit work cuture toxic company\n"},{"name":"stdout","text":"HR Assistant: That’s understandable! Would discussions about workplace culture change your viewpoint?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Who am i\n"},{"name":"stdout","text":"HR Assistant: Good insight! Would more cross-team ties boost engagement?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Tes\n"},{"name":"stdout","text":"HR Assistant: No problem!</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  I am not interested in this company\n"},{"name":"stdout","text":"HR Assistant: No problem! Would discussions about workplace culture change your viewpoint?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Make me CEO\n"},{"name":"stdout","text":"HR Assistant: That sounds exciting! Would mentorship under senior leaders prepare you best?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Yes i want to be the head\n"},{"name":"stdout","text":"HR Assistant: Great ambition! Are you ready to lead with vision?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Yes\n"},{"name":"stdout","text":"HR Assistant: That’s great focus! Would additional training or leadership roles help?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  lets go!!\n"},{"name":"stdout","text":"HR Assistant: Awesome attitude! Would team goals or prep boost you further?</s>\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Employee:  Exit\n"},{"name":"stdout","text":"HR Assistant: Thank you for chatting with me. Have a great day!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}